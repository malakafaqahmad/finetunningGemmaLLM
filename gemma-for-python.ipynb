{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ffb692a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T04:14:05.905992Z",
     "iopub.status.busy": "2024-03-28T04:14:05.905611Z",
     "iopub.status.idle": "2024-03-28T04:14:05.910257Z",
     "shell.execute_reply": "2024-03-28T04:14:05.909413Z"
    },
    "papermill": {
     "duration": 0.014397,
     "end_time": "2024-03-28T04:14:05.912258",
     "exception": false,
     "start_time": "2024-03-28T04:14:05.897861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "# !pip install -q -U keras-nlp\n",
    "# !pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8dd51c",
   "metadata": {
    "papermill": {
     "duration": 0.00553,
     "end_time": "2024-03-28T04:14:05.923739",
     "exception": false,
     "start_time": "2024-03-28T04:14:05.918209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setting Backend Environment for Deep Learning Framework\n",
    "\n",
    "Now, let's set the backend environment for our deep learning framework. Depending on our requirements and preferences, we can choose between \"jax\", \"torch\", or \"tensorflow\" as the backend. \n",
    "\n",
    "In this case, we're setting the backend to \"jax\" using the `os.environ` module. This specifies that we want to use JAX as our deep learning framework. \n",
    "\n",
    "Additionally, to avoid memory fragmentation issues on the JAX backend, we're setting the `XLA_PYTHON_CLIENT_MEM_FRACTION` environment variable to \"1.00\". This ensures efficient memory usage during computation. \n",
    "\n",
    "Let's execute the code to configure our backend environment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97bc387c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T04:14:05.937502Z",
     "iopub.status.busy": "2024-03-28T04:14:05.936976Z",
     "iopub.status.idle": "2024-03-28T04:14:05.946855Z",
     "shell.execute_reply": "2024-03-28T04:14:05.946065Z"
    },
    "papermill": {
     "duration": 0.019162,
     "end_time": "2024-03-28T04:14:05.948702",
     "exception": false,
     "start_time": "2024-03-28T04:14:05.929540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
    "# Avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963e047",
   "metadata": {
    "papermill": {
     "duration": 0.005549,
     "end_time": "2024-03-28T04:14:05.959914",
     "exception": false,
     "start_time": "2024-03-28T04:14:05.954365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> ***With the backend environment set, we're ready to utilize the chosen deep learning framework efficiently for our tasks.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e81c5",
   "metadata": {
    "papermill": {
     "duration": 0.005465,
     "end_time": "2024-03-28T04:14:05.971099",
     "exception": false,
     "start_time": "2024-03-28T04:14:05.965634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading and Preprocessing Dataset\n",
    "\n",
    "Now, let's load our dataset from a CSV file and preprocess it for further analysis or model training.\n",
    "\n",
    "We start by importing necessary libraries including Keras and Keras-NLP for deep learning tasks, as well as pandas for data manipulation.\n",
    "\n",
    "Next, we load the dataset from the specified CSV file (\"/kaggle/input/comprehensive-coding-dataset-for-gemma-finetunnig/prompts_126.csv\") into a pandas DataFrame `df`.\n",
    "\n",
    "After loading the data, we extract values from a single column into a list named `values` using pandas' `iloc` method.\n",
    "\n",
    "For demonstration purposes, we print the first few values of the list. Adjust the slice as needed based on the dataset size.\n",
    "\n",
    "Then, we convert the list to a set to remove duplicate values and ensure uniqueness. This set is then converted back to a list for further processing.\n",
    "\n",
    "Finally, we limit the number of values to 41500 by slicing the list, ensuring a manageable dataset size for model training.\n",
    "\n",
    "Let's execute the code to load and preprocess our dataset:\n",
    "\n",
    "\n",
    "With the dataset loaded and preprocessed, we're now ready to proceed with our analysis or model training using the cleaned data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6319c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T04:14:05.983794Z",
     "iopub.status.busy": "2024-03-28T04:14:05.983255Z",
     "iopub.status.idle": "2024-03-28T04:14:19.949325Z",
     "shell.execute_reply": "2024-03-28T04:14:19.948519Z"
    },
    "papermill": {
     "duration": 13.9748,
     "end_time": "2024-03-28T04:14:19.951684",
     "exception": false,
     "start_time": "2024-03-28T04:14:05.976884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 04:14:10.639664: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-28 04:14:10.639756: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-28 04:14:10.772397: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11210b3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T04:14:19.965792Z",
     "iopub.status.busy": "2024-03-28T04:14:19.964899Z",
     "iopub.status.idle": "2024-03-28T04:14:22.693938Z",
     "shell.execute_reply": "2024-03-28T04:14:22.693003Z"
    },
    "papermill": {
     "duration": 2.738347,
     "end_time": "2024-03-28T04:14:22.696177",
     "exception": false,
     "start_time": "2024-03-28T04:14:19.957830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Instruction:\\nyou are given the following instruction that describes the task.You are required to Write a python code that appropriately completes the request.\\n Create a function to calculate the sum of a sequence of integers. with the inputs [1, 2, 3, 4, 5]\\n\\nResponse:\\n# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"/kaggle/input/comprehensive-coding-dataset-for-gemma-finetunnig/prompts_126.csv\")\n",
    "\n",
    "# Extract values from the single column into a list\n",
    "values = df.iloc[:, 0].tolist()\n",
    "\n",
    "# Print the first few values as an example\n",
    "print(values[:1])  # Adjust the slice as ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac97cbad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T04:14:22.709664Z",
     "iopub.status.busy": "2024-03-28T04:14:22.709357Z",
     "iopub.status.idle": "2024-03-28T04:14:22.819075Z",
     "shell.execute_reply": "2024-03-28T04:14:22.818148Z"
    },
    "papermill": {
     "duration": 0.118654,
     "end_time": "2024-03-28T04:14:22.820943",
     "exception": false,
     "start_time": "2024-03-28T04:14:22.702289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(values))\n",
    "values = set(values)\n",
    "values = list(values)\n",
    "values = values[:15000]\n",
    "len(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78575599",
   "metadata": {
    "papermill": {
     "duration": 0.005992,
     "end_time": "2024-03-28T04:14:22.834211",
     "exception": false,
     "start_time": "2024-03-28T04:14:22.828219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Building and Using Gemma's Causal Language Model (LM)\n",
    "\n",
    "Now, let's utilize Gemma's Causal Language Model (LM) to generate responses to specific prompts or instructions. We'll use the model to demonstrate its capabilities in generating text based on provided instructions.\n",
    "\n",
    "First, we initialize Gemma's Causal LM using the preset \"gemma_2b_en\" through the `keras_nlp.models.GemmaCausalLM.from_preset` method. This preset is pre-trained on English text and suitable for generating natural language responses.\n",
    "\n",
    "Then, we display a summary of the model architecture using the `summary()` method to get an overview of its structure.\n",
    "\n",
    "Next, we define templates for instructions and responses. These templates will be used to format the prompts and model-generated responses for clarity.\n",
    "\n",
    "We provide specific instructions as prompts to the model, requesting it to generate responses based on the given prompts.\n",
    "\n",
    "Let's execute the code to initialize the model, generate responses to instructions, and observe the model's outputs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebfe37f",
   "metadata": {
    "papermill": {
     "duration": 0.005872,
     "end_time": "2024-03-28T04:14:22.846248",
     "exception": false,
     "start_time": "2024-03-28T04:14:22.840376",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> /kaggle/input/gemma/keras/gemma_2b_en/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff47f4eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T04:14:22.859722Z",
     "iopub.status.busy": "2024-03-28T04:14:22.858988Z",
     "iopub.status.idle": "2024-03-28T04:15:25.051399Z",
     "shell.execute_reply": "2024-03-28T04:15:25.050531Z"
    },
    "papermill": {
     "duration": 62.201109,
     "end_time": "2024-03-28T04:15:25.053277",
     "exception": false,
     "start_time": "2024-03-28T04:14:22.852168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "417cccb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T04:15:25.070305Z",
     "iopub.status.busy": "2024-03-28T04:15:25.070002Z",
     "iopub.status.idle": "2024-03-28T04:15:25.073932Z",
     "shell.execute_reply": "2024-03-28T04:15:25.073079Z"
    },
    "papermill": {
     "duration": 0.014612,
     "end_time": "2024-03-28T04:15:25.075769",
     "exception": false,
     "start_time": "2024-03-28T04:15:25.061157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e63971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T04:15:25.092760Z",
     "iopub.status.busy": "2024-03-28T04:15:25.092210Z",
     "iopub.status.idle": "2024-03-28T04:15:41.411289Z",
     "shell.execute_reply": "2024-03-28T04:15:41.410084Z"
    },
    "papermill": {
     "duration": 16.330173,
     "end_time": "2024-03-28T04:15:41.413597",
     "exception": false,
     "start_time": "2024-03-28T04:15:25.083424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "generate a python code to find the least number of steps needed to reach a target number given the following set of arithmetic operations: addition (+), subtraction (-), multiplication (*), division (/).\n",
      "\n",
      "Response:\n",
      "import math\n",
      "def find_steps(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    elif n == 2:\n",
      "        return 2\n",
      "    elif n == 3:\n",
      "        return 3\n",
      "    elif n == 4:\n",
      "        return 4\n",
      "    elif n == 5:\n",
      "        return 5\n",
      "    elif n == 6:\n",
      "        return 6\n",
      "    elif n == 7:\n",
      "        return 7\n",
      "    elif n == 8:\n",
      "        return 8\n",
      "    elif n == 9:\n",
      "        return 9\n",
      "    elif n == 10:\n",
      "        return 10\n",
      "    elif n == 11:\n",
      "        return 11\n",
      "    elif n == 12:\n",
      "        return 12\n",
      "    elif n == 13:\n",
      "        return 13\n",
      "    elif n == 14:\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"generate a python code to find the least number of steps needed to reach a target number given the following set of arithmetic operations: addition (+), subtraction (-), multiplication (*), division (/).\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df6bee4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T04:15:41.431068Z",
     "iopub.status.busy": "2024-03-28T04:15:41.430751Z",
     "iopub.status.idle": "2024-03-28T04:15:43.042823Z",
     "shell.execute_reply": "2024-03-28T04:15:43.041442Z"
    },
    "papermill": {
     "duration": 1.623174,
     "end_time": "2024-03-28T04:15:43.044899",
     "exception": false,
     "start_time": "2024-03-28T04:15:41.421725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "Create a program to find the most repeated character in an input string.\n",
      "\n",
      "Response:\n",
      "The program should accept a string as input and print the most repeated character in the string.\n",
      "\n",
      "Input:\n",
      "The program should accept a string as input.\n",
      "\n",
      "Output:\n",
      "The program should print the most repeated character in the string.\n",
      "\n",
      "Sample Input:\n",
      "Hello\n",
      "Sample Output:\n",
      "l\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"Create a program to find the most repeated character in an input string.\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36113ba6",
   "metadata": {
    "papermill": {
     "duration": 0.007937,
     "end_time": "2024-03-28T04:15:43.060950",
     "exception": false,
     "start_time": "2024-03-28T04:15:43.053013",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> ***With Gemma's Causal LM initialized and responses generated, we can observe the model's capabilities in generating text based on provided instructions, showcasing its potential for various natural language processing tasks.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117b5c9",
   "metadata": {
    "papermill": {
     "duration": 0.007695,
     "end_time": "2024-03-28T04:15:43.076581",
     "exception": false,
     "start_time": "2024-03-28T04:15:43.068886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuring Gemma's Causal Language Model (LM) with Advanced Options\n",
    "\n",
    "Now, let's configure Gemma's Causal LM with advanced options to enhance its performance and control memory usage.\n",
    "\n",
    "Firstly, we enable LoRA (Local Rank Adaptation) for the model and set the LoRA rank to 4 using the `enable_lora()` method.\n",
    "\n",
    "After enabling LoRA, we display the updated model summary to reflect the changes.\n",
    "\n",
    "Next, we set the input sequence length to 512 to control memory usage during training.\n",
    "\n",
    "Then, we specify the optimizer for training. Here, we use AdamW, a common optimizer for transformer models, with a learning rate of 5e-5 and weight decay of 0.01.\n",
    "\n",
    "We exclude layernorm and bias terms from weight decay to prevent unnecessary decay.\n",
    "\n",
    "After configuring the optimizer, we compile the model with the specified loss function, optimizer, and metrics.\n",
    "\n",
    "Finally, we train the model on the provided dataset `values` for one epoch with a batch size of 1.\n",
    "\n",
    "Let's execute the code to configure Gemma's Causal LM with these advanced options and train the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3aa10da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T04:15:43.094034Z",
     "iopub.status.busy": "2024-03-28T04:15:43.093367Z",
     "iopub.status.idle": "2024-03-28T04:15:43.575620Z",
     "shell.execute_reply": "2024-03-28T04:15:43.574655Z"
    },
    "papermill": {
     "duration": 0.493183,
     "end_time": "2024-03-28T04:15:43.577541",
     "exception": false,
     "start_time": "2024-03-28T04:15:43.084358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "894dca4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T04:15:43.597444Z",
     "iopub.status.busy": "2024-03-28T04:15:43.597149Z",
     "iopub.status.idle": "2024-03-28T07:18:17.194617Z",
     "shell.execute_reply": "2024-03-28T07:18:17.193635Z"
    },
    "papermill": {
     "duration": 10953.609506,
     "end_time": "2024-03-28T07:18:17.196532",
     "exception": false,
     "start_time": "2024-03-28T04:15:43.587026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15000/15000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10951s\u001b[0m 729ms/step - loss: 0.2557 - sparse_categorical_accuracy: 0.8229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7acc0c0c6fe0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = 512\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "gemma_lm.fit(values, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b6d326",
   "metadata": {
    "papermill": {
     "duration": 1.283074,
     "end_time": "2024-03-28T07:18:19.786709",
     "exception": false,
     "start_time": "2024-03-28T07:18:18.503635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> ***With Gemma's Causal LM configured with advanced options and trained on the provided dataset, it's now ready for further usage and experimentation.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64b4cd00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T07:18:22.357277Z",
     "iopub.status.busy": "2024-03-28T07:18:22.356886Z",
     "iopub.status.idle": "2024-03-28T07:18:40.370417Z",
     "shell.execute_reply": "2024-03-28T07:18:40.369254Z"
    },
    "papermill": {
     "duration": 19.240715,
     "end_time": "2024-03-28T07:18:40.372604",
     "exception": false,
     "start_time": "2024-03-28T07:18:21.131889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "Create a program to find the most repeated character in an input string.\n",
      "\n",
      "Response:\n",
      "def findMostRepeatedCharacter(str): \n",
      "    \n",
      "    # Initialize count \n",
      "    count = {} \n",
      "    max_count = 0 \n",
      "    most_repeated_char = '' \n",
      "  \n",
      "    for c in str: \n",
      "        if c not in count: \n",
      "            # If character not found \n",
      "            count[c] = 1 \n",
      "        else: \n",
      "            # If character is already found \n",
      "            count[c] += 1 \n",
      "  \n",
      "        if count[c] > max_count: \n",
      "            # Update maximum count \n",
      "            max_count = max(count.values()) \n",
      "            most_repeated_char = c \n",
      "  \n",
      "    # Return the maximum count \n",
      "    return max_count, most_repeated_char \n",
      "\n",
      "if __name__ == '__main__': \n",
      "    str = 'geeksforgeeks' \n",
      "    print(findMostRepeatedCharacter(str))\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"Create a program to find the most repeated character in an input string.\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "738fc146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T07:18:42.896475Z",
     "iopub.status.busy": "2024-03-28T07:18:42.895632Z",
     "iopub.status.idle": "2024-03-28T07:18:47.844542Z",
     "shell.execute_reply": "2024-03-28T07:18:47.843530Z"
    },
    "papermill": {
     "duration": 6.2562,
     "end_time": "2024-03-28T07:18:47.846846",
     "exception": false,
     "start_time": "2024-03-28T07:18:41.590646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "Develop an algorithm to find the least number of steps needed to reach a target number given the following set of arithmetic operations: addition (+), subtraction (-), multiplication (*), division (/).\n",
      "\n",
      "Response:\n",
      "function findSteps(a, b, c, target) {\n",
      "    if (a === target && b === 0) return 0;\n",
      "    if (a === target && b === target + 1) return 1;\n",
      "    if (b === target + 1 && c === target - 1) return 2;\n",
      "    if (a === 0) return Infinity;\n",
      "\n",
      "    const steps = Math.min(Math.min(findSteps(a - c, b, c, target), findSteps(a, b, c + 1, target)), findSteps(a, b - c + 1, c, target));\n",
      "    return steps === Infinity ? -1 : steps + 2; \n",
      "}\n",
      "\n",
      "findSteps(0, 0, 0, 100);\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"Develop an algorithm to find the least number of steps needed to reach a target number given the following set of arithmetic operations: addition (+), subtraction (-), multiplication (*), division (/).\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58d98e9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T07:18:50.402103Z",
     "iopub.status.busy": "2024-03-28T07:18:50.401738Z",
     "iopub.status.idle": "2024-03-28T07:18:54.206864Z",
     "shell.execute_reply": "2024-03-28T07:18:54.205865Z"
    },
    "papermill": {
     "duration": 5.12643,
     "end_time": "2024-03-28T07:18:54.208945",
     "exception": false,
     "start_time": "2024-03-28T07:18:49.082515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "generate a python code to find the least number of steps needed to reach a target number given the following set of arithmetic operations: addition (+), subtraction (-), multiplication (*), division (/).\n",
      "\n",
      "Response:\n",
      "def findSteps(targetNumber, operations):\n",
      "    steps = 0\n",
      "    while targetNumber != 0:\n",
      "        steps += 1\n",
      "        if operations[targetNumber - 1] == '+':\n",
      "            targetNumber -= 1\n",
      "            steps += 1\n",
      "        elif operations[targetNumber - 1] == '*':\n",
      "            targetNumber *= 2\n",
      "            steps += 1\n",
      "        elif operations[targetNumber - 1] == '/':\n",
      "                targetNumber //= 2\n",
      "                steps += 1\n",
      "\n",
      "        else:\n",
      "            targetNumber -= 2\n",
      "            steps += 1\n",
      "        \n",
      "    return steps\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"generate a python code to find the least number of steps needed to reach a target number given the following set of arithmetic operations: addition (+), subtraction (-), multiplication (*), division (/).\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4670783,
     "sourceId": 7943973,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5171,
     "sourceId": 11371,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11095.73087,
   "end_time": "2024-03-28T07:18:58.826696",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-28T04:14:03.095826",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
